{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14d1102e",
   "metadata": {},
   "source": [
    "# E-08 뉴스 요약하기 \n",
    "## Step 1. 데이터 수집하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60fc9883",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /aiffel/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upGrad learner switches to career in ML & Al with 90% salary hike\n",
      "Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94231</th>\n",
       "      <td>105-year-old man rides roller coaster, sets wo...</td>\n",
       "      <td>A 105-year-old great-grandfather from the UK h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79163</th>\n",
       "      <td>Karnataka university replaces bouquets with kh...</td>\n",
       "      <td>Sri Siddhartha University in Karnataka's Tumak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84184</th>\n",
       "      <td>India is Afghanistan's most reliable regional ...</td>\n",
       "      <td>India is Afghanistan's most reliable regional ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26724</th>\n",
       "      <td>Let paparazzi guess if papa is raazi:Ã¢ÂÂMah...</td>\n",
       "      <td>Filmmaker Mahesh Bhatt, while commenting on hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85485</th>\n",
       "      <td>Lendingkart raises Ã¢ÂÂ¹50 crore debt from Ye...</td>\n",
       "      <td>Fintech startup Lendingkart has raised a debt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56684</th>\n",
       "      <td>Rape-accused actor's medical data shows he's g...</td>\n",
       "      <td>According to reports, the medical reports of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11701</th>\n",
       "      <td>BigBasket may raise $400 mn, become a billion-...</td>\n",
       "      <td>Grocery startup BigBasket is in talks to raise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54410</th>\n",
       "      <td>Karni Sena blames underworld for CBFC's 'Padma...</td>\n",
       "      <td>Rajput organisation Karni Sena's member Sukhde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5613</th>\n",
       "      <td>Man accidentally shoots at friend while showin...</td>\n",
       "      <td>A 20-year-old man got injured in his stomach a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31981</th>\n",
       "      <td>Iran deal had been 'recipe for disaster': Isra...</td>\n",
       "      <td>Praising US President Donald Trump's \"brave an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               headlines  \\\n",
       "94231  105-year-old man rides roller coaster, sets wo...   \n",
       "79163  Karnataka university replaces bouquets with kh...   \n",
       "84184  India is Afghanistan's most reliable regional ...   \n",
       "26724  Let paparazzi guess if papa is raazi:Ã¢ÂÂMah...   \n",
       "85485  Lendingkart raises Ã¢ÂÂ¹50 crore debt from Ye...   \n",
       "56684  Rape-accused actor's medical data shows he's g...   \n",
       "11701  BigBasket may raise $400 mn, become a billion-...   \n",
       "54410  Karni Sena blames underworld for CBFC's 'Padma...   \n",
       "5613   Man accidentally shoots at friend while showin...   \n",
       "31981  Iran deal had been 'recipe for disaster': Isra...   \n",
       "\n",
       "                                                    text  \n",
       "94231  A 105-year-old great-grandfather from the UK h...  \n",
       "79163  Sri Siddhartha University in Karnataka's Tumak...  \n",
       "84184  India is Afghanistan's most reliable regional ...  \n",
       "26724  Filmmaker Mahesh Bhatt, while commenting on hi...  \n",
       "85485  Fintech startup Lendingkart has raised a debt ...  \n",
       "56684  According to reports, the medical reports of t...  \n",
       "11701  Grocery startup BigBasket is in talks to raise...  \n",
       "54410  Rajput organisation Karni Sena's member Sukhde...  \n",
       "5613   A 20-year-old man got injured in his stomach a...  \n",
       "31981  Praising US President Donald Trump's \"brave an...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import urllib.request\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/sunnysai12345/News_Summary/master/news_summary_more.csv\", filename=\"news_summary_more.csv\")\n",
    "data = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')\n",
    "print(data['headlines'][0])\n",
    "print(data['text'][0])\n",
    "\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4ec787",
   "metadata": {},
   "source": [
    "## step 2. 데이터 전처리하기 (추상적 요약)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a35d24",
   "metadata": {},
   "source": [
    "- 추상적 요약 :  원문으로부터 내용이 요약된 새로운 문장을 생성. 여기서 새로운 문장이라는 것은 결과로 나온 문장이 원문에 원래 없던 문장일 수도 있다는 것을 의미합니다. 자연어 처리 분야 중 자연어 생성(Natural Language Generation, NLG)의 영역 .\n",
    "- 추출적 요약 : 원문을 구성하는 문장 중 어느 것이 요약문에 들어갈 핵심문장인지를 판별한다는 점에서 '문장 분류'(Text Classification) 문제로 볼 수 있을 것입니다.(네이버 뉴스 요약봇)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bccbcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 열에서 중복을 배제한 유일한 샘플의 수 : 98360\n",
      "Summary 열에서 중복을 배제한 유일한 샘플의 수 : 98280\n"
     ]
    }
   ],
   "source": [
    "print('Text 열에서 중복을 배제한 유일한 샘플의 수 :', data['text'].nunique())\n",
    "print('Summary 열에서 중복을 배제한 유일한 샘플의 수 :', data['headlines'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f887e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 98360\n"
     ]
    }
   ],
   "source": [
    "data.drop_duplicates(subset = ['text'], inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36e7c225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "headlines    0\n",
      "text         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56ace532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정규화 사전의 수:  120\n"
     ]
    }
   ],
   "source": [
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "print(\"정규화 사전의 수: \", len(contractions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e466125d",
   "metadata": {},
   "source": [
    "- 텍스트 정규화(text normalization)를 위한 사전\n",
    "- 사전은 아래의 링크에서 참고하여 만들었어요.\n",
    "https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a322065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 개수 : 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /aiffel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "print('불용어 개수 :', len(stopwords.words('english') ))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "668479a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 함수\n",
    "def preprocess_sentence(sentence, remove_stopwords=True):\n",
    "    sentence = sentence.lower() # 텍스트 소문자화\n",
    "    sentence = BeautifulSoup(sentence, \"lxml\").text # <br />, <a href = ...> 등의 html 태그 제거\n",
    "    sentence = re.sub(r'\\([^)]*\\)', '', sentence) # 괄호로 닫힌 문자열 (...) 제거 Ex) my husband (and myself!) for => my husband for\n",
    "    sentence = re.sub('\"','', sentence) # 쌍따옴표 \" 제거\n",
    "    sentence = ' '.join([contractions[t] if t in contractions else t for t in sentence.split(\" \")]) # 약어 정규화\n",
    "    sentence = re.sub(r\"'s\\b\",\"\", sentence) # 소유격 제거. Ex) roland's -> roland\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence) # 영어 외 문자(숫자, 특수문자 등) 공백으로 변환\n",
    "    sentence = re.sub('[m]{2,}', 'mm', sentence) # m이 3개 이상이면 2개로 변경. Ex) ummmmmmm yeah -> umm yeah\n",
    "    \n",
    "    # 불용어 제거 (Text)\n",
    "    if remove_stopwords:\n",
    "        tokens = ' '.join(word for word in sentence.split() if not word in stopwords.words('english') if len(word) > 1)\n",
    "    # 불용어 미제거 (Summary)\n",
    "    else:\n",
    "        tokens = ' '.join(word for word in sentence.split() if len(word) > 1)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "752aba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장을 받고 전처리르 한다음 불용어는 split으로 나눠서 불용어에 있지 않다면 그 단어들을 다시 꺼내서 붙이는 형식 \n",
    "#headlines는 짧기 때문에 불용어를 제거하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3684456b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.\n"
     ]
    }
   ],
   "source": [
    "print(data['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d35eab4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 전처리 후 결과:  ['saurav kant alumnus upgrad iiit pg program machine learning artificial intelligence sr systems engineer infosys almost years work experience program upgrad degree career support helped transition data scientist tech mahindra salary hike upgrad online power learning powered lakh careers', 'kunal shah credit card bill payment platform cred gave users chance win free food swiggy one year pranav kaushik delhi techie bagged reward spending cred coins users get one cred coin per rupee bill paid used avail rewards brands like ixigo bookmyshow ubereats cult fit', 'new zealand defeated india wickets fourth odi hamilton thursday win first match five match odi series india lost international match rohit sharma captaincy consecutive victories dating back march match witnessed india getting seventh lowest total odi cricket history', 'aegon life iterm insurance plan customers enjoy tax benefits premiums paid save taxes plan provides life cover age years also customers options insure critical illnesses disability accidental death benefit rider life cover age years', 'speaking sexual harassment allegations rajkumar hirani sonam kapoor said known hirani many years true metoo movement get derailed metoo movement always believe woman case need reserve judgment added hirani accused assistant worked sanju']\n",
      "headlines 전처리 후 결과:  ['upgrad learner switches to career in ml al with salary hike', 'delhi techie wins free food from swiggy for one year on cred', 'new zealand end rohit sharma led india match winning streak', 'aegon life iterm insurance plan helps customers save tax', 'have known hirani for yrs what if metoo claims are not true sonam']\n"
     ]
    }
   ],
   "source": [
    "clean_text = []\n",
    "# 전체 Text 데이터에 대한 전처리 : 10분 이상 시간이 걸릴 수 있습니다. \n",
    "for s in data['text']:\n",
    "    clean_text.append(preprocess_sentence(s))\n",
    "\n",
    "# 전처리 후 출력\n",
    "print(\"Text 전처리 후 결과: \", clean_text[:5])\n",
    "\n",
    "clean_headlines = []\n",
    "for s in data['headlines']:\n",
    "    clean_headlines.append(preprocess_sentence(s, False))\n",
    "\n",
    "print(\"headlines 전처리 후 결과: \", clean_headlines[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9165c02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 98360\n",
      "=3\n"
     ]
    }
   ],
   "source": [
    "tmp = data.copy()\n",
    "data['text'] = clean_text\n",
    "data['headlines'] = clean_headlines\n",
    "data.isnull().sum()\n",
    "data.dropna(axis=0, inplace=True)\n",
    "print('전체 샘플수 :', (len(data)))\n",
    "# 빈 값을 Null 값으로 변환\n",
    "data.replace('', np.nan, inplace=True)\n",
    "print('=3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96ba2501",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp에 데이터 원본 보존 \n",
    "#전처리한 텍스트와 헤드라인을 데이터에 집어넣는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a6a7912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텍스트의 최소 길이 : 1\n",
      "텍스트의 최대 길이 : 60\n",
      "텍스트의 평균 길이 : 35.09968483123221\n",
      "요약의 최소 길이 : 1\n",
      "요약의 최대 길이 : 16\n",
      "요약의 평균 길이 : 9.299532330215534\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 길이 분포 출력\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_len = [len(s.split()) for s in data['text']]\n",
    "summary_len = [len(s.split()) for s in data['headlines']]\n",
    "\n",
    "print('텍스트의 최소 길이 : {}'.format(np.min(text_len)))\n",
    "print('텍스트의 최대 길이 : {}'.format(np.max(text_len)))\n",
    "print('텍스트의 평균 길이 : {}'.format(np.mean(text_len)))\n",
    "print('요약의 최소 길이 : {}'.format(np.min(summary_len)))\n",
    "print('요약의 최대 길이 : {}'.format(np.max(summary_len)))\n",
    "print('요약의 평균 길이 : {}'.format(np.mean(summary_len)))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(text_len)\n",
    "plt.title('Text')\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(summary_len)\n",
    "plt.title('Headlines')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.title('Text')\n",
    "plt.hist(text_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()\n",
    "\n",
    "plt.title('headlines')\n",
    "plt.hist(summary_len, bins = 40)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d93c69d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#텍스트와 헤드라인의 문장길이를 확인\n",
    "#텍스트는 40 헤드라인은 10정도로 임의로 잡았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67776d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def below_threshold_len(max_len, nested_list):\n",
    "    cnt = 0\n",
    "    for s in nested_list:\n",
    "        if(len(s.split()) <= max_len):\n",
    "            cnt = cnt + 1\n",
    "    print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582089fe",
   "metadata": {},
   "source": [
    "각각 40과 10로 정했는데 이 길이를 선택했을 때, 얼마나 많은 샘플들을 자르지 않고 포함할 수 있는지 통계로 확인하는 편이 객관적으로 길이를 결정하는 데 도움이 될거예요. 훈련 데이터와 샘플의 길이를 입력하면, 데이터의 몇 %가 해당하는지 계산하는 함수를 만들어서 좀 더 정확하게 판단해볼게요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04f69cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 중 길이가 40 이하인 샘플의 비율: 0.9238714924766165\n",
      "전체 샘플 중 길이가 10 이하인 샘플의 비율: 0.8162972753151687\n"
     ]
    }
   ],
   "source": [
    "text_max_len = 40\n",
    "headlines_max_len = 10\n",
    "\n",
    "## 비율확인\n",
    "below_threshold_len(text_max_len, data['text'])\n",
    "below_threshold_len(headlines_max_len,  data['headlines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7306a0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플수 : 74102\n"
     ]
    }
   ],
   "source": [
    "#정해진 길이에 맞춰 자르는 것이 아니라, 정해진 길이보다 길면 제외하는 방법으로 데이터를 정제\n",
    "data = data[data['text'].apply(lambda x: len(x.split()) <= text_max_len)]\n",
    "data = data[data['headlines'].apply(lambda x: len(x.split()) <= headlines_max_len)]\n",
    "print('전체 샘플수 :', (len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "764ca950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "      <th>decoder_input</th>\n",
       "      <th>decoder_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new zealand end rohit sharma led india match w...</td>\n",
       "      <td>new zealand defeated india wickets fourth odi ...</td>\n",
       "      <td>sostoken new zealand end rohit sharma led indi...</td>\n",
       "      <td>new zealand end rohit sharma led india match w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aegon life iterm insurance plan helps customer...</td>\n",
       "      <td>aegon life iterm insurance plan customers enjo...</td>\n",
       "      <td>sostoken aegon life iterm insurance plan helps...</td>\n",
       "      <td>aegon life iterm insurance plan helps customer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rahat fateh ali khan denies getting notice for...</td>\n",
       "      <td>pakistani singer rahat fateh ali khan denied r...</td>\n",
       "      <td>sostoken rahat fateh ali khan denies getting n...</td>\n",
       "      <td>rahat fateh ali khan denies getting notice for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cong wins ramgarh bypoll in rajasthan takes to...</td>\n",
       "      <td>congress candidate shafia zubair ramgarh assem...</td>\n",
       "      <td>sostoken cong wins ramgarh bypoll in rajasthan...</td>\n",
       "      <td>cong wins ramgarh bypoll in rajasthan takes to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>up cousins fed human excreta for friendship wi...</td>\n",
       "      <td>two minor cousins uttar pradesh gorakhpur alle...</td>\n",
       "      <td>sostoken up cousins fed human excreta for frie...</td>\n",
       "      <td>up cousins fed human excreta for friendship wi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headlines  \\\n",
       "2   new zealand end rohit sharma led india match w...   \n",
       "3   aegon life iterm insurance plan helps customer...   \n",
       "5   rahat fateh ali khan denies getting notice for...   \n",
       "9   cong wins ramgarh bypoll in rajasthan takes to...   \n",
       "10  up cousins fed human excreta for friendship wi...   \n",
       "\n",
       "                                                 text  \\\n",
       "2   new zealand defeated india wickets fourth odi ...   \n",
       "3   aegon life iterm insurance plan customers enjo...   \n",
       "5   pakistani singer rahat fateh ali khan denied r...   \n",
       "9   congress candidate shafia zubair ramgarh assem...   \n",
       "10  two minor cousins uttar pradesh gorakhpur alle...   \n",
       "\n",
       "                                        decoder_input  \\\n",
       "2   sostoken new zealand end rohit sharma led indi...   \n",
       "3   sostoken aegon life iterm insurance plan helps...   \n",
       "5   sostoken rahat fateh ali khan denies getting n...   \n",
       "9   sostoken cong wins ramgarh bypoll in rajasthan...   \n",
       "10  sostoken up cousins fed human excreta for frie...   \n",
       "\n",
       "                                       decoder_target  \n",
       "2   new zealand end rohit sharma led india match w...  \n",
       "3   aegon life iterm insurance plan helps customer...  \n",
       "5   rahat fateh ali khan denies getting notice for...  \n",
       "9   cong wins ramgarh bypoll in rajasthan takes to...  \n",
       "10  up cousins fed human excreta for friendship wi...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 요약 데이터에는 시작 토큰과 종료 토큰을 추가한다.\n",
    "data['decoder_input'] = data['headlines'].apply(lambda x : 'sostoken '+ x)\n",
    "data['decoder_target'] = data['headlines'].apply(lambda x : x + ' eostoken')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48ca486d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = np.array(data['text']) # 인코더의 입력\n",
    "decoder_input = np.array(data['decoder_input']) # 디코더의 입력\n",
    "decoder_target = np.array(data['decoder_target']) # 디코더의 레이블\n",
    "\n",
    "#np.array로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "074e0266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74102\n",
      "[45775   570 12376 ... 56206 62633 73666]\n"
     ]
    }
   ],
   "source": [
    "#numpy 모듈의 arange 함수는 반열린구간 [start, stop) 에서 step 의 크기만큼 일정하게 떨어져 있는 숫자들을 array 형태로 반환해 주는 함수.\n",
    "print(encoder_input.shape[0])\n",
    "indices = np.arange(encoder_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "#encoder_input과 크기와 형태가 같은 순서가 섞인 정수 시퀀스 = indices.\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9f66583",
   "metadata": {},
   "outputs": [],
   "source": [
    "#샘플들을 랜덤하게 잘섞기\n",
    "encoder_input = encoder_input[indices]\n",
    "decoder_input = decoder_input[indices]\n",
    "decoder_target = decoder_target[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ccafbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터의 수 : 14820\n",
      "훈련 데이터의 개수 : 59282\n",
      "훈련 레이블의 개수 : 59282\n",
      "테스트 데이터의 개수 : 14820\n",
      "테스트 레이블의 개수 : 14820\n"
     ]
    }
   ],
   "source": [
    "#훈련 데이터 0.8 테스트 데이터 0.2 \n",
    "n_of_val = int(len(encoder_input)*0.2)\n",
    "print('테스트 데이터의 수 :', n_of_val)\n",
    "#0~0.8\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "#0.8~1.0\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b004f7",
   "metadata": {},
   "source": [
    "- 이 텍스트를 숫자로 처리할 수 있도록 훈련 데이터와 테스트 데이터의 단어들을 모두 정수로 바꿔주기. \n",
    "- 각 단어에 고유한 정수를 맵핑하는 작업 => 단어 집합(vocabulary) 만들기\n",
    "- 훈련 데이터에 대해서 단어 집합 = encoder_input_train에 \n",
    "\n",
    "Keras의 토크나이저를 사용하여, 입력된 훈련 데이터로부터 단어 집합을 만들었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6b17646",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = Tokenizer() # 토크나이저 정의\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 입력된 데이터로부터 단어 집합 생성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fab6c85",
   "metadata": {},
   "source": [
    "- 등장 빈도수가 7회 미만인 단어들이 이 데이터에서 얼만큼의 비중을 차지하는지 확인\n",
    "- src_tokenizer.word_counts.items()에는 단어와 각 단어의 등장 빈도수가 저장되어있어서 이것을 통해 통계적인 확인가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69ea81a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 61786\n",
      "등장 빈도가 6번 이하인 희귀 단어의 수: 42475\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 19311\n",
      "단어 집합에서 희귀 단어의 비율: 68.74534684232674\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 4.206604041934793\n"
     ]
    }
   ],
   "source": [
    "threshold = 7\n",
    "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2be2d84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 61786\n",
      "등장 빈도가 9번 이하인 희귀 단어의 수: 45885\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 15901\n",
      "단어 집합에서 희귀 단어의 비율: 74.26439646521867\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 5.520054762893358\n"
     ]
    }
   ],
   "source": [
    "threshold = 10\n",
    "total_cnt = len(src_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in src_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94aa866",
   "metadata": {},
   "source": [
    "- threashold를 10으로 해도 희귀 단어 등장 빈도 비율이 5.48밖에 안되니 7-> 10으로 해주었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7838322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.71 프로로 등장하는 5이하의 빈도로 나타나는 단어들을 삭제\n",
    "src_vocab = 15000\n",
    "src_tokenizer = Tokenizer(num_words=src_vocab) # 단어 집합의 크기를 150000으로 제한\n",
    "src_tokenizer.fit_on_texts(encoder_input_train) # 단어 집합 재생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29b4b298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[144, 3421, 2474, 4893, 73, 729, 2603, 1728, 93, 10329, 402, 301, 1463, 1638, 189, 142, 1, 434, 2603, 708, 1728, 2993, 9020, 8611, 8611, 2208, 7525, 9, 708, 4643], [8281, 641, 4059, 2363, 226, 1503, 2889, 790, 2234, 486, 425, 820, 738, 618, 226, 1503, 202, 3365, 11783, 673, 192, 314, 41, 5430, 3743, 302, 337, 3932, 412, 320, 820, 738, 673, 372, 9021, 2925, 1843, 2499, 1], [426, 237, 6647, 2870, 11371, 57, 585, 29, 2735, 2701, 1546, 141, 810, 93, 2408, 9778, 33, 2701, 1546, 2735, 810, 241, 153, 237, 55, 6076, 72, 5582, 5926, 14368, 242, 4060, 4136, 52, 5582, 11784]]\n"
     ]
    }
   ],
   "source": [
    "# texts_to_sequences()는 생성된 단어 집합에 기반하여 입력으로 주어진 텍스트 데이터의 단어들을 모두 정수로 변환하는 정수 인코딩을 수행. \n",
    "# 현재 단어 집합의 크기를 15000으로 제한했으니까 이제 15000이 넘는 숫자들은 정수 인코딩 후에는 데이터에 존재하지 않아요\n",
    "encoder_input_train = src_tokenizer.texts_to_sequences(encoder_input_train) \n",
    "encoder_input_test = src_tokenizer.texts_to_sequences(encoder_input_test)\n",
    "\n",
    "# 잘 진행되었는지 샘플 출력\n",
    "print(encoder_input_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46d5ca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_tokenizer = Tokenizer()\n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "#타겟에 대해서도 동일한 작업을 수행할게요. \n",
    "#케라스의 토크나이저를 사용하여 decoder_input_train을 입력으로 전체 단어 집합과 각 단어에 대한 빈도수를 계산해요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a64f1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 27359\n",
      "등장 빈도가 5번 이하인 희귀 단어의 수: 18295\n",
      "단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 9064\n",
      "단어 집합에서 희귀 단어의 비율: 66.87013414232976\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 5.984328719355027\n"
     ]
    }
   ],
   "source": [
    "threshold = 6\n",
    "total_cnt = len(tar_tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tar_tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :', total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print('단어 집합에서 희귀 단어를 제외시킬 경우의 단어 집합의 크기 %s'%(total_cnt - rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7061b5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "input  [[1, 301, 8228, 81, 558, 12, 3674, 2801, 4, 4490], [1, 4885, 522, 542, 89, 1731, 346, 443, 131, 501, 666], [1, 6, 1732, 5129, 6, 1995, 200, 45], [1, 136, 1054, 378, 106, 15, 3675, 4, 1853, 46, 431], [1, 56, 1854, 3208, 7, 14, 3019, 4, 835, 319]]\n",
      "target\n",
      "decoder  [[301, 8228, 81, 558, 12, 3674, 2801, 4, 4490, 2], [4885, 522, 542, 89, 1731, 346, 443, 131, 501, 666, 2], [6, 1732, 5129, 6, 1995, 200, 45, 2], [136, 1054, 378, 106, 15, 3675, 4, 1853, 46, 431, 2], [56, 1854, 3208, 7, 14, 3019, 4, 835, 319, 2]]\n"
     ]
    }
   ],
   "source": [
    "tar_vocab = 9000\n",
    "tar_tokenizer = Tokenizer(num_words=tar_vocab) \n",
    "tar_tokenizer.fit_on_texts(decoder_input_train)\n",
    "tar_tokenizer.fit_on_texts(decoder_target_train)\n",
    "\n",
    "# 텍스트 시퀀스를 정수 시퀀스로 변환\n",
    "decoder_input_train = tar_tokenizer.texts_to_sequences(decoder_input_train) \n",
    "decoder_target_train = tar_tokenizer.texts_to_sequences(decoder_target_train)\n",
    "decoder_input_test = tar_tokenizer.texts_to_sequences(decoder_input_test)\n",
    "decoder_target_test = tar_tokenizer.texts_to_sequences(decoder_target_test)\n",
    "\n",
    "# 잘 변환되었는지 확인\n",
    "print('input')\n",
    "print('input ',decoder_input_train[:5])\n",
    "print('target')\n",
    "print('decoder ',decoder_target_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e58d370",
   "metadata": {},
   "source": [
    "# 정수인코딩을 끝낸 뒤 빈문장을 없애주기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e115b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "삭제할 훈련 데이터의 개수 : 0\n",
      "삭제할 테스트 데이터의 개수 : 1\n",
      "훈련 데이터의 개수 : 59282\n",
      "훈련 레이블의 개수 : 59282\n",
      "테스트 데이터의 개수 : 14819\n",
      "테스트 레이블의 개수 : 14819\n"
     ]
    }
   ],
   "source": [
    "drop_train = [index for index, sentence in enumerate(decoder_input_train) if len(sentence) == 1]\n",
    "drop_test = [index for index, sentence in enumerate(decoder_input_test) if len(sentence) == 1]\n",
    "\n",
    "print('삭제할 훈련 데이터의 개수 :', len(drop_train))\n",
    "print('삭제할 테스트 데이터의 개수 :', len(drop_test))\n",
    "\n",
    "encoder_input_train = [sentence for index, sentence in enumerate(encoder_input_train) if index not in drop_train]\n",
    "decoder_input_train = [sentence for index, sentence in enumerate(decoder_input_train) if index not in drop_train]\n",
    "decoder_target_train = [sentence for index, sentence in enumerate(decoder_target_train) if index not in drop_train]\n",
    "\n",
    "encoder_input_test = [sentence for index, sentence in enumerate(encoder_input_test) if index not in drop_test]\n",
    "decoder_input_test = [sentence for index, sentence in enumerate(decoder_input_test) if index not in drop_test]\n",
    "decoder_target_test = [sentence for index, sentence in enumerate(decoder_target_test) if index not in drop_test]\n",
    "\n",
    "print('훈련 데이터의 개수 :', len(encoder_input_train))\n",
    "print('훈련 레이블의 개수 :', len(decoder_input_train))\n",
    "print('테스트 데이터의 개수 :', len(encoder_input_test))\n",
    "print('테스트 레이블의 개수 :', len(decoder_input_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b29d137",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_train = pad_sequences(encoder_input_train, maxlen=text_max_len, padding='post')\n",
    "encoder_input_test = pad_sequences(encoder_input_test, maxlen=text_max_len, padding='post')\n",
    "decoder_input_train = pad_sequences(decoder_input_train, maxlen=headlines_max_len, padding='post')\n",
    "decoder_target_train = pad_sequences(decoder_target_train, maxlen=headlines_max_len, padding='post')\n",
    "decoder_input_test = pad_sequences(decoder_input_test, maxlen=headlines_max_len, padding='post')\n",
    "decoder_target_test = pad_sequences(decoder_target_test, maxlen=headlines_max_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "47aa2b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "# 인코더 설계 시작\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "\n",
    "# 인코더\n",
    "encoder_inputs = Input(shape=(text_max_len,))\n",
    "\n",
    "# 인코더의 임베딩 층\n",
    "enc_emb = Embedding(src_vocab, embedding_dim)(encoder_inputs)\n",
    "\n",
    "# 인코더의 LSTM 1\n",
    "encoder_lstm1 = LSTM(hidden_size, return_sequences=True, return_state=True ,dropout = 0.4, recurrent_dropout = 0.4)\n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
    "\n",
    "# 인코더의 LSTM 2\n",
    "encoder_lstm2 = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
    "\n",
    "# 인코더의 LSTM 3\n",
    "\n",
    "encoder_lstm3 = LSTM(hidden_size, return_state=True, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae971e03",
   "metadata": {},
   "source": [
    "- LSTM 장점 : LSTM은 dropout 뿐 아니라 recurrent dropout까지 사용. 일반적인 dropout은 레이어의 weight를 랜덤으로 생략하여 모델의 과적합(overfitting)을 해결해주는 방법\n",
    "\n",
    "- recurrent dropout :  dropout을 레이어가 아닌 time step마다 해주는 방식. time step의 입력을 랜덤으로 생략해 주는 거죠. recurrent dropout은 일반적인 dropout와 같이 regularization을 해주는 효과가 있고, 과적합을 방지할 수 있다.\n",
    "\n",
    "- 이 모델에선 두가지 다 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fef9e38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "# 디코더 설계\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "\n",
    "# 디코더의 임베딩 층\n",
    "dec_emb_layer = Embedding(tar_vocab, embedding_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 디코더의 LSTM\n",
    "decoder_lstm = LSTM(hidden_size, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04b55c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 40, 128)      1920000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 40, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 40, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    1152000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 40, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 9000)   2313000     lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 7,224,104\n",
      "Trainable params: 7,224,104\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_outputs) \n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5a29a2",
   "metadata": {},
   "source": [
    "어텐션 메커니즘\n",
    "어텐션 메커니즘을 수행하는 어텐션 함수를 설계하는 것은 또 다른 새로운 신경망을 설계해야 한다는 뜻이에요. 어텐션 함수를 설계해보는 것은 다음 기회로 미루기로 하고, 여기서는 TensorFlow에 이미 구현된 어텐션 함수를 가져와서 디코더의 출력층에 어떤 방식으로 결합하는지 배워볼게요. 참고로 여기서 사용하는 어텐션 함수는 Bahdanau 스타일의 어텐션입니다. 이 어텐션에 대한 자세한 설명은 텐서플로우 홈페이지를 참고하세요.\n",
    "\n",
    "아래와 같이 어텐션 층을 만들고, 위에서 설계한 디코더의 출력층을 수정해 봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b3b33",
   "metadata": {},
   "source": [
    "## Step 3. 어텐션 메커니즘 사용하기 (추상적 요약)\n",
    "일반적인 seq2seq보다는 어텐션 메커니즘을 사용한 seq2seq를 사용하는 것이 더 나은 성능을 얻을 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5285ba60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 40, 128)      1920000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 40, 256), (N 394240      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 40, 256), (N 525312      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 128)    1152000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 40, 256), (N 525312      lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  394240      embedding_1[0][0]                \n",
      "                                                                 lstm_2[0][1]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AdditiveAttent (None, None, 256)    256         lstm_3[0][0]                     \n",
      "                                                                 lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 512)    0           lstm_3[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 9000)   4617000     concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 9,528,360\n",
      "Trainable params: 9,528,360\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import AdditiveAttention\n",
    "\n",
    "# 어텐션 층(어텐션 함수)\n",
    "attn_layer = AdditiveAttention(name='attention_layer')\n",
    "\n",
    "# 인코더와 디코더의 모든 time step의 hidden state를 어텐션 층에 전달하고 결과를 리턴\n",
    "attn_out = attn_layer([decoder_outputs, encoder_outputs])\n",
    "\n",
    "\n",
    "# 어텐션의 결과와 디코더의 hidden state들을 연결\n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_softmax_layer = Dense(tar_vocab, activation='softmax')\n",
    "decoder_softmax_outputs = decoder_softmax_layer(decoder_concat_input)\n",
    "\n",
    "# 모델 정의\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c85a840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "232/232 [==============================] - 145s 557ms/step - loss: 6.3139 - val_loss: 5.8796\n",
      "Epoch 2/50\n",
      "232/232 [==============================] - 127s 549ms/step - loss: 5.7892 - val_loss: 5.5438\n",
      "Epoch 3/50\n",
      "232/232 [==============================] - 128s 552ms/step - loss: 5.4504 - val_loss: 5.2917\n",
      "Epoch 4/50\n",
      "232/232 [==============================] - 128s 550ms/step - loss: 5.1792 - val_loss: 5.0629\n",
      "Epoch 5/50\n",
      "232/232 [==============================] - 128s 551ms/step - loss: 4.9376 - val_loss: 4.9024\n",
      "Epoch 6/50\n",
      "232/232 [==============================] - 127s 547ms/step - loss: 4.7379 - val_loss: 4.7741\n",
      "Epoch 7/50\n",
      "232/232 [==============================] - 127s 547ms/step - loss: 4.5689 - val_loss: 4.6631\n",
      "Epoch 8/50\n",
      "232/232 [==============================] - 128s 550ms/step - loss: 4.4187 - val_loss: 4.5900\n",
      "Epoch 9/50\n",
      "232/232 [==============================] - 127s 549ms/step - loss: 4.2806 - val_loss: 4.5045\n",
      "Epoch 10/50\n",
      "232/232 [==============================] - 127s 548ms/step - loss: 4.1574 - val_loss: 4.4454\n",
      "Epoch 11/50\n",
      "232/232 [==============================] - 127s 549ms/step - loss: 4.0458 - val_loss: 4.3992\n",
      "Epoch 12/50\n",
      "232/232 [==============================] - 127s 547ms/step - loss: 3.9445 - val_loss: 4.3577\n",
      "Epoch 13/50\n",
      "232/232 [==============================] - 127s 547ms/step - loss: 3.8486 - val_loss: 4.3285\n",
      "Epoch 14/50\n",
      "232/232 [==============================] - 127s 546ms/step - loss: 3.7594 - val_loss: 4.2986\n",
      "Epoch 15/50\n",
      "232/232 [==============================] - 127s 546ms/step - loss: 3.6780 - val_loss: 4.2759\n",
      "Epoch 16/50\n",
      "232/232 [==============================] - 127s 546ms/step - loss: 3.6032 - val_loss: 4.2547\n",
      "Epoch 17/50\n",
      "232/232 [==============================] - 127s 547ms/step - loss: 3.5311 - val_loss: 4.2436\n",
      "Epoch 18/50\n",
      "232/232 [==============================] - 127s 547ms/step - loss: 3.4656 - val_loss: 4.2308\n",
      "Epoch 19/50\n",
      "232/232 [==============================] - 127s 548ms/step - loss: 3.4026 - val_loss: 4.2122\n",
      "Epoch 20/50\n",
      "232/232 [==============================] - 127s 548ms/step - loss: 3.3424 - val_loss: 4.2017\n",
      "Epoch 21/50\n",
      "232/232 [==============================] - 127s 548ms/step - loss: 3.2849 - val_loss: 4.1956\n",
      "Epoch 22/50\n",
      "232/232 [==============================] - 127s 547ms/step - loss: 3.2260 - val_loss: 4.2077\n",
      "Epoch 23/50\n",
      "232/232 [==============================] - 127s 546ms/step - loss: 3.1789 - val_loss: 4.1852\n",
      "Epoch 24/50\n",
      "232/232 [==============================] - 127s 546ms/step - loss: 3.1320 - val_loss: 4.1948\n",
      "Epoch 25/50\n",
      "232/232 [==============================] - 128s 551ms/step - loss: 3.0858 - val_loss: 4.1922\n",
      "Epoch 00025: early stopping\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
    "es = EarlyStopping(monitor='val_loss', patience=2, verbose=1)\n",
    "history = model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "          validation_data=([encoder_input_test, decoder_input_test], decoder_target_test), \\\n",
    "          batch_size=256, callbacks=[es], epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d01e38d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_index_to_word = src_tokenizer.index_word # 원문 단어 집합에서 정수 -> 단어를 얻음\n",
    "tar_word_to_index = tar_tokenizer.word_index # 요약 단어 집합에서 단어 -> 정수를 얻음\n",
    "tar_index_to_word = tar_tokenizer.index_word # 요약 단어 집합에서 정수 -> 단어를 얻음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d37618",
   "metadata": {},
   "source": [
    "# 인퍼런스 단계 모델 설계\n",
    "\n",
    "- seq2seq는 훈련할 때와 실제 동작할 때(인퍼런스 단계)의 방식이 다르므로 그에 맞게 모델 설계를 별개로 진행해\n",
    "\n",
    "- 훈련 단계에서는 디코더의 입력부에 정답이 되는 문장 전체를 한꺼번에 넣고 디코더의 출력과 한 번에 비교할 수 있으므로, 인코더와 디코더를 엮은 통짜 모델 하나만 준비했습니다.\n",
    "\n",
    "- 그러나 정답 문장이 없는 인퍼런스 단계에서는 만들어야 할 문장의 길이만큼 디코더가 반복 구조로 동작해야 하기 때문에 부득이하게 인퍼런스를 위한 모델 설계를 별도로 해주어야 합니다. 이때는 인코더 모델과 디코더 모델을 분리해서 설계합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34c9996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 설계\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# 이전 시점의 상태들을 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(hidden_size,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size,))\n",
    "\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# 문장의 다음 단어를 예측하기 위해서 초기 상태(initial_state)를 이전 시점의 상태로 사용. 이는 뒤의 함수 decode_sequence()에 구현\n",
    "# 훈련 과정에서와 달리 LSTM의 리턴하는 은닉 상태와 셀 상태인 state_h와 state_c를 버리지 않음.\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6fe05393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 어텐션 함수\n",
    "decoder_hidden_state_input = Input(shape=(text_max_len, hidden_size))\n",
    "attn_out_inf = attn_layer([decoder_outputs2, decoder_hidden_state_input])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# 디코더의 출력층\n",
    "decoder_outputs2 = decoder_softmax_layer(decoder_inf_concat) \n",
    "\n",
    "# 최종 디코더 모델\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4d3a5ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "     # <SOS>에 해당하는 토큰 생성\n",
    "    target_seq = np.zeros((1,1))\n",
    "    target_seq[0, 0] = tar_word_to_index['sostoken']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition: # stop_condition이 True가 될 때까지 루프 반복\n",
    "\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = tar_index_to_word[sampled_token_index]\n",
    "\n",
    "        if (sampled_token!='eostoken'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        #  <eos>에 도달하거나 최대 길이를 넘으면 중단.\n",
    "        if (sampled_token == 'eostoken'  or len(decoded_sentence.split()) >= (headlines_max_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # 길이가 1인 타겟 시퀀스를 업데이트\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # 상태를 업데이트 합니다.\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aecf765a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2text(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if (i!=0):\n",
    "            temp = temp + src_index_to_word[i]+' '\n",
    "    return temp\n",
    "\n",
    "# 요약문의 정수 시퀀스를 텍스트 시퀀스로 변환\n",
    "def seq2summary(input_seq):\n",
    "    temp=''\n",
    "    for i in input_seq:\n",
    "        if ((i!=0 and i!=tar_word_to_index['sostoken']) and i!=tar_word_to_index['eostoken']):\n",
    "            temp = temp + tar_index_to_word[i] + ' '\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c598bc65",
   "metadata": {},
   "source": [
    "## Step 4. 실제 결과와 요약문 비교하기 (추상적 요약)\n",
    "원래의 요약문(headlines 열)과 학습을 통해 얻은 추상적 요약의 결과를 비교해 보세요.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d06ced86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 : jat groups haryana called rally jind reportedly aimed disrupting bjp president amit shah bike rally decision came six hour meeting cm manohar lal khattar assured jat leaders taking steps fulfil reservation demands recently government withdrew cases filed jat agitation \n",
      "실제 요약 : call off stir to disrupt amit shah haryana rally \n",
      "예측 요약 :  amit shah shah outside amit shah office\n",
      "\n",
      "\n",
      "원문 : us based first time mother flying germany denial pregnancy claimed deliver baby hotel room turkey watching youtube year old experienced labour pains mid flight gave birth freeman broke news family baby boy seven weeks old \n",
      "실제 요약 : us woman delivers baby herself by watching online \n",
      "예측 요약 :  baby baby baby baby baby baby baby baby girl\n",
      "\n",
      "\n",
      "원문 : yoga guru ramdev patanjali said kimbho app removed google play store launched trial basis one day patanjali spokesperson sk said official launch app technical work still progress app garnered within three hours launch added \n",
      "실제 요약 : it was trial version patanjali on app \n",
      "예측 요약 :  patanjali to launch new app for messenger\n",
      "\n",
      "\n",
      "원문 : year old girl us city detroit suffered serious burns attempting fire challenge viral challenge involves people pouring liquid bodies lighting fire jumping water flames suffered second third degree burns ventilator \n",
      "실제 요약 : year old us girl burned during fire challenge \n",
      "예측 요약 :  teen steals fire in us city\n",
      "\n",
      "\n",
      "원문 : talking bjp uttar pradesh civic polls victory party national general secretary ram madhav said feel like opposition parties fun case feel government something wrong opposition raise matter added also commented india pakistan relationship pakistan change \n",
      "실제 요약 : no fun not having good opposition bjp leader ram \n",
      "예측 요약 :  bjp should take states in tripura bjp leader\n",
      "\n",
      "\n",
      "원문 : aircraft carrying passengers four crew members crash landed northern ireland making emergency landing nose gear failed plane forced circle city dump fuel order reduce weight landing nose runway fire engines rushed towards passengers evacuated safely \n",
      "실제 요약 : plane crash lands at airport after nose fails \n",
      "예측 요약 :  plane carrying emergency landing after engine hits the engine\n",
      "\n",
      "\n",
      "원문 : according reports actors sanjay dutt sridevi star together upcoming film years two actors last seen together film film produced karan johar dharma productions reportedly also feature alia bhatt varun dhawan sonakshi sinha \n",
      "실제 요약 : after yrs sridevi dutt to star together in film report \n",
      "예측 요약 :  release date of salman khan starrer star announced\n",
      "\n",
      "\n",
      "원문 : nike world largest athletic gear company said would cut jobs also reduce number clothing styles makes part restructuring programme said plans sell shoes directly customers online nike said able offer products customers faster \n",
      "실제 요약 : nike to cut jobs reduce shoe \n",
      "예측 요약 :  microsoft to pay lakh to for\n",
      "\n",
      "\n",
      "원문 : official teaser horror film nun spin released film focuses character introduced stars oscar nominated jonas among others directed hardy nun scheduled released september \n",
      "실제 요약 : teaser of the spin off the nun released \n",
      "예측 요약 :  teaser of kangana ranaut starrer released\n",
      "\n",
      "\n",
      "원문 : rti activist valmiki yadav friend shot dead bike borne assailants sunday night bihar yadav exposed several financial irregularities public welfare schemes development work district third reported incident three months rti activist killed bihar \n",
      "실제 요약 : rti activist shot dead in bihar rd murder in months \n",
      "예측 요약 :  former bihar minister shot dead in up\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"원문 :\", seq2text(encoder_input_test[i]))\n",
    "    print(\"실제 요약 :\", seq2summary(decoder_input_test[i]))\n",
    "    print(\"예측 요약 :\", decode_sequence(encoder_input_test[i].reshape(1, text_max_len)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956b82db",
   "metadata": {},
   "source": [
    "원문 : swiss food drink company tuesday said closed deal market us coffee giant starbucks products around world outside company coffee shops billion deal gives rights sell starbucks products starbucks employees us europe join deal \n",
    "실제 요약 : closes billion deal to market starbucks products \n",
    "예측 요약 :  starbucks to acquire starbucks starbucks for billion\n",
    "추상적으로 추출하다보니 가짜 뉴스를 만들기도한다. \n",
    "\n",
    "원문 : least people died people missing diving boat people capsized due storm thailand resort island thursday chinese tourists thai crew members tour onboard boat accident occurred police warned businesses taking boats sea \n",
    "실제 요약 : die as boat tourists capsizes in thailand \n",
    "예측 요약 :  dead after boat capsizes in boat capsizes in\n",
    "\n",
    "둘다 전복되어 사망한 사건에 대해 요약을 한다. \n",
    "\n",
    "10가지 정도를 보았는데 위의 요약말고는 제대로 되었다고 말하는것이 애매하다. \n",
    "\n",
    "미리 학습된 임베딩층을 쓰던, 모델을 변경시켜야 될 것같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f07785d",
   "metadata": {},
   "source": [
    "## Step 5. Summa을 이용해서 추출적 요약해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e7ca4a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from summa.summarizer import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81abc477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Saurav Kant, an alumnus of upGrad and IIIT-B's...\n",
       "1        Kunal Shah's credit card bill payment platform...\n",
       "2        New Zealand defeated India by 8 wickets in the...\n",
       "3        With Aegon Life iTerm Insurance plan, customer...\n",
       "4        Speaking about the sexual harassment allegatio...\n",
       "                               ...                        \n",
       "98396    A CRPF jawan was on Tuesday axed to death with...\n",
       "98397    'Uff Yeh', the first song from the Sonakshi Si...\n",
       "98398    According to reports, a new version of the 199...\n",
       "98399    A new music video shows rapper Snoop Dogg aimi...\n",
       "98400    Madhesi Morcha, an alliance of seven political...\n",
       "Name: text, Length: 98401, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_news = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')\n",
    "data_news[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e5f07bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict_Headlines:\n",
      "Two minor cousins in Uttar Pradesh's Gorakhpur were allegedly repeatedly burnt with tongs and forced to eat human excreta by their family for being friends with two boys from the same school.\n"
     ]
    }
   ],
   "source": [
    "print('Predict_Headlines:')\n",
    "print(summarize(data_news[\"text\"][10], ratio=0.5,split=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e7652aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict_Headlines:\n",
      "[\"Two minor cousins in Uttar Pradesh's Gorakhpur were allegedly repeatedly burnt with tongs and forced to eat human excreta by their family for being friends with two boys from the same school.\"]\n"
     ]
    }
   ],
   "source": [
    "print('Predict_Headlines:')\n",
    "print(summarize(data_news[\"text\"][10], ratio=0.5, split=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a90d46e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict_Headlines:\n",
      "Two minor cousins in Uttar Pradesh's Gorakhpur were allegedly repeatedly burnt with tongs and forced to eat human excreta by their family for being friends with two boys from the same school.\n",
      "The cousins revealed their ordeal to the police and Child Welfare Committee after being brought back to Gorakhpur from Nepal, where they had fled to escape the torture.\n"
     ]
    }
   ],
   "source": [
    "print('Predict_Headlines:')\n",
    "print(summarize(data_news[\"text\"][10], words=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c9090b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original :\n",
      "Two minor cousins in Uttar Pradesh's Gorakhpur were allegedly repeatedly burnt with tongs and forced to eat human excreta by their family for being friends with two boys from the same school. The cousins revealed their ordeal to the police and Child Welfare Committee after being brought back to Gorakhpur from Nepal, where they had fled to escape the torture.\n",
      "Headlines :\n",
      "UP cousins fed human excreta for friendship with boys\n"
     ]
    }
   ],
   "source": [
    "print('Original :')\n",
    "print(data_news[\"text\"][10])\n",
    "print('Headlines :')\n",
    "print(data_news[\"headlines\"][10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc423a18",
   "metadata": {},
   "source": [
    "\n",
    "### summarize\n",
    "- 결과로 나온 요약 문장은 인풋텍스트 중 가장 대표적인 문장으로 구성되며, 줄 바꿈으로 나뉘어서 문자열로 반환\n",
    "\n",
    "Input\n",
    "\n",
    "- 인풋은 string 타입이여야한다.\n",
    "- 납득이 될 만한 요약을 위해서는 INPUT_MIN_LENGTH보다 길이가 길어야한다. \n",
    "- 텍스트는 젠심에 있는 split_sentences 매서드를 사용하여 문장이 나눠질 것이다.\n",
    " Parameters \n",
    "- text (str) : 인풋 텍스트\n",
    "- ratio (float, optional) : 0과 1사이의 수로써 요약을 위해 선택되는 원본 텍스트 문장의 수의 비율을 결정한다.\n",
    "- words (int or None, optional) : 아웃풋에 얼마나 많은 단어를 출력시킬지 정한다. 만약에 ratio와 함께 주어진다면 ratio는 무시된다.\n",
    "- split (bool, optional) : True인 경우, 문장의 리스트가 리턴된다. 그렇지 않을 경우는 string이 리턴된다.\n",
    "\n",
    " \n",
    "예측_헤드라인:\n",
    "\"우타르프라데시주 고라흐푸르에서 두 명의 사촌이 같은 학교 남학생 2명과 친구라는 이유로 반복적으로 집게에 화상을 입고 가족들에 의해 인간의 배설물을 먹도록 강요받았다고 한다.\n",
    "\n",
    "예측_헤드라인:\n",
    "우타르프라데시주 고라흐푸르에 사는 사촌 동생 2명은 같은 학교 남학생 2명과 친구라는 이유로 집게에 불을 지르고 가족들에 의해 인간의 배설물을 먹도록 강요받았다.\n",
    "사촌들은 고문을 피해 네팔에서 고라크푸르로 돌아온 후 경찰과 아동복지위원회에 그들의 고통을 털어놓았다.\n",
    "\n",
    "원본 :\n",
    "우타르프라데시주 고라흐푸르에 사는 사촌 동생 2명은 같은 학교 남학생 2명과 친구라는 이유로 집게에 불을 지르고 가족들에 의해 인간의 배설물을 먹도록 강요받았다. 사촌들은 고문을 피해 네팔에서 고라크푸르로 돌아온 후 경찰과 아동복지위원회에 그들의 고통을 털어놓았다.\n",
    "\n",
    "헤드라인:\n",
    "사촌들이 소년들과의 우정 때문에 인간의 배설물을 먹였다.\n",
    "\n",
    "\n",
    "## 예측 헤드라인과 원본 헤드라인이 결이 비슷하다. 하지만 추출이라 새로운 단어는 만들지는 못한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3924c8f",
   "metadata": {},
   "source": [
    "# 회고\n",
    "\n",
    "- seq2seq와 attention 매커니즘을 사용한 추상적 요약이 새로운 단어들을 만들어 요약을 하긴하지만 안맞는 경우도 많았다.\n",
    "- 이것을 해결해주기 위해 임베딩레이어를 학습된 word2vec 가져와서 교체해보려고 하였으나 학습단계와 인퍼런스단계의 완벽히 이해하지 못해 둘다 임베딩 레이어를 교체해줘야하는지 아니면 (아마도 그럴것같긴하지만) 다르게 해야하는지 정확하게 몰라서 시도를 하지 못했다.\n",
    "- RNN , lstm , 인퍼런스 단계 모델, 어텐션 메커니즘의 내부적인 수학적 동작은 이해를 완벽하게 하지는 못했으나 개념이나 사용이유에 대해선 이해했다. \n",
    "- 전에 RNN관련 EP를 할때는 Padding을할때 pad_sequence함수로 max_len을 조절해서 패딩길이만 조절했는데, 이전에 전처리를 안해줘 글자가 잘리는 경우가 있었는거같다. 이번에 그것에 대한 언급이 있어 잘 배운거같다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
